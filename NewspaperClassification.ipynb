{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNK/CZtwTEa7rOOLSMAhJt8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srv-sh/BanglaNewspaperClassification/blob/main/NewspaperClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa7komiEEtal"
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz_CznyKFGIt"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "# পারমিশন দেখে নিন\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vErOv2XXF5mp"
      },
      "source": [
        "!kaggle datasets download -d furcifer/bangla-newspaper-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSJEouRuF81M"
      },
      "source": [
        "!unzip bangla-newspaper-dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bGGWWv1GQlS"
      },
      "source": [
        "import json \n",
        "with open('data_v2/data_v2.json',encoding = 'utf-8') as f:\n",
        "  datastore = json.load(f)\n",
        "sentences = []\n",
        "labels = []\n",
        "for item in datastore:\n",
        "  sentences.append(item['content'])\n",
        "  labels.append(item['category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaFK_V4uGwng"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF8n_lxdHxXl"
      },
      "source": [
        "#lebels \n",
        "set(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqC4sUX2ILzY"
      },
      "source": [
        "# Encoding the labels \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "label = encoder.fit_transform(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYQCzPtbJJVU"
      },
      "source": [
        "training_size = int(len(sentences)*.88)\n",
        "\n",
        "training_sentences = sentences[:training_size]\n",
        "training_labels = label[:training_size]\n",
        "test_sentences = sentences[training_size:]\n",
        "test_labels = label[training_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPYgnsVnJV_-"
      },
      "source": [
        "print(len(training_sentences))\n",
        "print(len(test_sentences))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWAuPlNYKllF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#tokenize dataset\n",
        "tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om-mMquTLPiv"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD3lxbRmVrBI"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwBYPEeSWAlL"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl0EskWbcQy1"
      },
      "source": [
        " maxin= [len(training_sentences[i]) for i in range(len(training_sentences))]\n",
        " max_length = max(maxin)\n",
        " max_length = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W_0NdchWB2q"
      },
      "source": [
        "#padding \n",
        "\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences,max_length,padding=padding_type,truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "teesting_padded = pad_sequences(testing_sequences,maxlen=max_length,padding=padding_type,truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XwOhMqZEtAs"
      },
      "source": [
        "print(testing_sequences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dZMTIv3NH63"
      },
      "source": [
        "#this conversion is for tensorflow\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(teesting_padded)\n",
        "testing_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HOgSpiGN8Me"
      },
      "source": [
        "embedding_dim = 12\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzv9UiT8Oaku"
      },
      "source": [
        "model =  tf.keras.Sequential([\n",
        "                              tf.keras.layers.Embedding(input_dim=vocab_size, \n",
        "                              output_dim=embedding_dim, \n",
        "                              input_length=max_length),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "                              tf.keras.layers.Dense(9, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zw2KgvROcJ9"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRGNHxRcOkEv"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ89ZGU3OnC4"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6GWqGNUOqp6"
      },
      "source": [
        "num_epochs = 5\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), callbacks=[tensorboard_callback], verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnykCfTBOswq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}